PaddlePaddle、飞桨、论文复现

Few-shot Video-to-Video Synthesis


vid2vid主要用于视频的合成转化，主要的目的是将输入的语义视频，例如：用户姿态、分段蒙版视频，转化为具有真实感的视频。
现有的方法存在两个局限性：1）训练过程需要大量的数据（目标人体或者场景）；2）学习的模型具有优先的泛化能力，经过训练的模型只能用于合成与训练集中的视频相似的，无法适用于未知的场景。

主要工作：
1.提出一个小样本vid2vid框架，该框架通过在测试时利用目标的少量示例图像来学习合成以前没见过的主题或场景的视频。
2.模型利用注意力机制的新型网络权重生成模块实现快速的泛化能力。

相关工作：
1.	GANs：使用条件GAN框架，根据用户输入数据生成输出，从而可以更灵活的控制输出。此外，还采用一组示例图像，用于测试阶段，通过一个新的网络权重生成模块动态确定所提出的小样本vid2vid模型的网络权重。
2.	图像到图像的合成：将输入图像从一个领域转移到另一个领域中的对应图像。文章的目标是视频合成，并通过网络权重生成方案将其推广到未知场景中。
3.	视频生成模型：文章主要涉及将语义输入视频转换为具有真实感的视频，可以通过利用测试时提供的少量示例图像来合成未知场景的视频。
4.	自适应网络：根据输入数据动态计算部分权重的网络。
5.	人体姿势转移：利用处于不同姿势的人的图片来合成，文章主要特点是不使用特定的人体先验。

小样本的vid2vid模型
vid2vid模型目标是学习可以转换语义图像序列的映射函数，所生成的训练器模型可以用于转换新的输入语义视频，但无法用于合成未知场景的视频。
小样本vid2vid2模型提出网络权重生成模块，用于提取模式。网络权重生成模块仅生成空间调制分支的权重，可以大大减少网络权重生成模块所必须生成的参数数量，从而避免了过拟合问题。其次，避免产生从示例图像到输出图像的快捷通道。
网络权重生成模块E分别由注意力网络EA,示例特征提取器EF和多层感知器EF三个子网络模块构成。


实验：
数据集主要采用：
1）	YouTube舞蹈视频：有1500个来自YouTube的跳舞视频组成；
2）	街头现场视频：来自三个不同地理区域的街道现场视频：德国（Cityscapes数据集）、波士顿（使用行车记录仪收集）和纽约（通过不同行车记录仪收集）；
3）	脸部说话视频：使用FaceForensics数据集中的真实视频，包含854个来自不同记者的新闻发布会
对比基线方法：
1）	ConcatStyle
2）	AdaIN
3）	PoseWrap
4）	MonkeyNet
评价指标：
1）	FID初始距离：测量实际数据的分布于生成数据之间的距离，用于量化合成图像的保真度
2）	Pose error：估计合成对象的姿势。

